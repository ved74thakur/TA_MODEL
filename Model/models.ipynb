{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca833315",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074009f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset containing summaries and captions\n",
    "    \"\"\"\n",
    "    # Assuming the data is in CSV format with columns: summary, caption, category\n",
    "    df = pd.read_csv(\"manual_annotations.csv\")\n",
    "    print(f\"Successfully loaded {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25744bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering functions\n",
    "def extract_features(df):\n",
    "    \"\"\"\n",
    "    Extract features from summary and caption pairs\n",
    "    \"\"\"\n",
    "    print(\"Extracting features...\")\n",
    "    \n",
    "    # Load pre-trained sentence transformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings for summaries and captions\n",
    "    summary_embeddings = model.encode(df['Summary'].tolist())\n",
    "    caption_embeddings = model.encode(df['Caption'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity between summary and caption\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(df)):\n",
    "        similarity = 1 - cosine(summary_embeddings[i], caption_embeddings[i])\n",
    "        cosine_similarities.append(similarity)\n",
    "    \n",
    "    # Create additional features\n",
    "    features = pd.DataFrame({\n",
    "        'cosine_similarity': cosine_similarities,\n",
    "        'summary_length': df['Summary'].apply(len),\n",
    "        'caption_length': df['Caption'].apply(len),\n",
    "        'length_ratio': df['Summary'].apply(len) / df['Caption'].apply(len),\n",
    "        'word_count_summary': df['Summary'].apply(lambda x: len(x.split())),\n",
    "        'word_count_caption': df['Caption'].apply(lambda x: len(x.split())),\n",
    "        'word_count_ratio': df['Summary'].apply(lambda x: len(x.split())) / df['Caption'].apply(lambda x: len(x.split())),\n",
    "    })\n",
    "    \n",
    "    # You could add more features here\n",
    "    return features, df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using stratified k-fold cross-validation\n",
    "    \"\"\"\n",
    "    # Initialize the stratified k-fold \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Define the models to try\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        'SVM': SVC(class_weight='balanced', probability=True),\n",
    "        'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100)\n",
    "    }\n",
    "    \n",
    "    # Metrics to evaluate\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "        'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "        'f1_macro': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining and evaluating {name}...\")\n",
    "        \n",
    "        # Apply SMOTE for handling class imbalance within a Pipeline\n",
    "        # using imblearn's Pipeline to correctly integrate SMOTE\n",
    "        from imblearn.pipeline import Pipeline # Import imblearn's Pipeline\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),  # Apply scaling first if necessary\n",
    "            # Adjust k_neighbors for SMOTE to handle smaller class sizes\n",
    "            ('smote', SMOTE(random_state=42, k_neighbors=min(5, len(np.unique(y)) - 1))), # Then apply SMOTE\n",
    "            ('classifier', model)  \n",
    "        ])\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_results = cross_validate(pipeline, X, y, \n",
    "                                    cv=skf, \n",
    "                                    scoring=scoring, \n",
    "                                    return_estimator=True,\n",
    "                                    return_train_score=True)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'cv_results': cv_results,\n",
    "            'accuracy': cv_results['test_accuracy'].mean(),\n",
    "            'precision': cv_results['test_precision_macro'].mean(),\n",
    "            'recall': cv_results['test_recall_macro'].mean(),\n",
    "            'f1': cv_results['test_f1_macro'].mean()\n",
    "        }\n",
    "        \n",
    "        print(f\"  Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "        print(f\"  Precision (macro): {results[name]['precision']:.4f}\")\n",
    "        print(f\"  Recall (macro): {results[name]['recall']:.4f}\")\n",
    "        print(f\"  F1 Score (macro): {results[name]['f1']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab332e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(results):\n",
    "    \"\"\"\n",
    "    Select the best model based on F1 macro score\n",
    "    \"\"\"\n",
    "    scores = {name: results[name]['f1'] for name in results}\n",
    "    best_model = max(scores, key=scores.get)\n",
    "    print(f\"\\nBest model: {best_model} with F1 macro score: {scores[best_model]:.4f}\")\n",
    "    return best_model, results[best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for detailed evaluation of the best model\n",
    "def detailed_evaluation(X, y, best_model_name, best_model_results):\n",
    "    \"\"\"\n",
    "    Perform detailed evaluation of the best model\n",
    "    \"\"\"\n",
    "    print(\"\\nDetailed evaluation of the best model...\")\n",
    "    \n",
    "    # Get the best estimator from cross-validation\n",
    "    best_estimator = best_model_results['cv_results']['estimator'][0]\n",
    "    \n",
    "    # Initialize a new StratifiedKFold for evaluation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=43)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Refit the pipeline on this fold\n",
    "        best_estimator.fit(X_train, y_train)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        \n",
    "        # Store for aggregation\n",
    "        all_predictions.extend(y_pred)\n",
    "        all_true_labels.extend(y_test)\n",
    "    \n",
    "    # Overall classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_true_labels, all_predictions))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=np.unique(y),\n",
    "                yticklabels=np.unique(y))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'classification_report': classification_report(all_true_labels, all_predictions, output_dict=True),\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d74c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X, best_model_results):\n",
    "    \"\"\"\n",
    "    Analyze feature importance for the best model (if applicable)\n",
    "    \"\"\"\n",
    "    best_estimator = best_model_results['cv_results']['estimator'][0]\n",
    "    \n",
    "    # Check if the model supports feature importance\n",
    "    if hasattr(best_estimator[-1], 'feature_importances_'):\n",
    "        importances = best_estimator[-1].feature_importances_\n",
    "        feature_names = X.columns\n",
    "        \n",
    "        # Create and plot feature importance\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    elif hasattr(best_estimator[-1], 'coef_'):\n",
    "        # For models like logistic regression\n",
    "        coefficients = best_estimator[-1].coef_\n",
    "        feature_names = X.columns\n",
    "        \n",
    "        # If multiclass, take average absolute coefficient\n",
    "        if len(coefficients.shape) > 1:\n",
    "            importances = np.mean(np.abs(coefficients), axis=0)\n",
    "        else:\n",
    "            importances = np.abs(coefficients)\n",
    "            \n",
    "        # Create and plot feature importance\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    else:\n",
    "        print(\"Feature importance not available for this model type\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
